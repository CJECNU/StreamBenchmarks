\section{Related work}
\label{rel}
%Now with emerging next generation stream data processing engines, batch processing is seen as a special case of stream processing where the data size is bounded. 
Benchmarking distributed data processing systems has been an active area of research. Huang et.al. proposed HiBench \cite{huang2011hibench}, which was the first benchmark suite to evaluate and characterize the performance of  Hadoop. In a more recent version, the benchmark also features a streaming component \cite{white2012hadoop}. The authors conduct a wide range of experiments from micro-benchmarks to machine learning algorithms. A highly related benchmark is SparkBench, which features machine learning, graph computation, SQL queries, and streaming applications on top of Apache Spark \cite{li2015sparkbench}.
Building an application level, end-to-end big data benchmark with all major characteristics in the lifecycle of big data systems  is the main goal of BigBench \cite{ghazal2013bigbench}.
Wang et al. introduce BigDataBench, the big data benchmark suite for Internet services. The BigDataBench suite contains  19 scenarios covering a broad range of applications and diverse and representative data sets \cite{wang2014bigdatabench}.

Benchmarks on SDPSs extend the batch data processing benchmarks and pose unique challenges. Recently, researchers from Yahoo! conducted a series of experiments on stream data processing engines to measure their latency and throughput \cite{chintapalli2016benchmarking}. They used Apache Kafka \cite{kafka2014high} and Redis \cite{carlson2013redis} for data fetching and storage. Later on, on the other hand, it was shown that those systems actually are a bottleneck for the tested systems' performance \cite{dataartisans}. In this paper we overcome this bottleneck and measure event time latency. Marcu et al. did an extensive analysis of the differences between Apache Spark and Apache Flink by correlating the execution plan with  resource utilization and  parameter configuration \cite{marcu2016spark}. The authors conduct experiments with batch and iterative workloads on up to 100 nodes. Their key finding is similar to ours that none of the  systems outperforms the other for all data types, sizes and job patterns. However, the authors leave benchmarking the SDP features of the systems  for the future work, which we present in this paper.

Perera et al. use Karamel \cite{karamel} to provide reproducible batch and SDP benchmarks of Apache Spark and Apache Flink in a cloud environment \cite{perera2016reproducible}.  We can identify the similarity in the results in Spark and Flink's memory usages between that particular work and  our paper. However, for other metrics such as CPU usage and latency, we notice significant differences. The authors used the  Yahoo stream benchmarks for evaluation and  it was shown above that the particular benchmarking framework have bottlenecks; thus, this can be a possible reason for differences in evaluation results. In Section \ref{chal} we analyze this challenge and provide our low-overhead  solution to overcome this bottleneck.
Lopez et al. propose a benchmarking framework to assess the fault tolerance and throughput efficiency for the open source stream data processing engines Storm, Spark and Flink \cite{lopez2016performance}. The key finding of the paper is that  the micro-batch stream processing system, Spark, is more robust to node failures, i.e., it efficiently stores the full processing state of the micro-batches and distributes the interrupted processing with low-overhead among other worker nodes.
On the other hand, Spark performs up to 15 times worse than Storm and Flink. Compared the particular paper with our work,  there are differences in terms of the throughputs of the systems. In our paper, we analyze the \textit{unreal throughput} as being one of the challenges and provide our solution, \textit{sustainable throughput}.   
Shukla et al. perform common IoT tasks with different stream data processing engines, evaluating their performance \cite{shukla2016benchmarking}. 
Firstly, the authors define the latency being an interval  between source operator's ingest and sink operator's emitting time. As we will discuss in Section \ref{chal}, this approach may lead to unrealistic results. Thus, we provide a solution for the event time latency.  Secondly, the authors define throughput  as the  rate of output messages emitted from the sink operators in unit time. However, the output throughput may not be useful in various real-life use-cases. In our paper, we introduce sustainable throughput and provide a low-overhead solution to measure it. 

Lu et al. propose StreamBench, a solution to measure the throughput and latency of a SDSP by putting mediator system between the data source and SUT \cite{lu2014stream}. In Section \ref{chal}, we show that the mediator system can be a bottleneck and resulting benchmarks may not exhibit precise results. Thus, we provide an efficient  solution to use local,  distributed, and efficient mediator system.  The LinearRoad benchmark was proposed by Arasu et al. to measure the performance of standalone stream data management systems such as Aurora \cite{abadi2003aurora} by simulating a toll system scenario for motor vehicle expressways \cite{arasu2004linear}. The authors use response time, which is the time difference between input's arrival and output from SDPS, as a metric. As stated above, this definition of latency can lead to unrealistic results; thus, we provide low-overhead solution to measure event time latency.
Several stream processing systems implement their own benchmarks to measure the system performance without comparing them with any other system \cite{neumeyer2010s4,qian2013timestream,zaharia2012discretized}. 

 \todo[inline]{This is not enough, you should add a differentiation for every or at least every class of related work. This needs to exactly point out, where other the work falls short=>fixed This could still be improved}
%As we overcome the bottlenecks faced in previous works, the resulting system gets simpler, which is one of key points in benchmarking.
 
 
