\section{Introduction}
Stream data processing has been gaining significant attention due to its wide range of uses in big data analytics. The main idea is that processing big volumes of data with batch processing engines is not enough anymore and data has to be processed fast to enable fast adaptation and reaction to changed conditions. Several engines are widely adopted and supported by open source community, such as Apache Storm \cite{toshniwal2014storm}, Apache Spark  \cite{zaharia2012discretized} , Apache Flink \cite{carbone2015apache}. 

One of application areas of stream data processing is  video games. Processing of large scale online data feeds from different sources swift is imperative for online video game companies in industry. For example, once the company releases the new feature of particular video game, fast and efficient analysis of customer usage statistics is a must to determine the their attitudes and satisfaction. In this paper, we collaborate with \textit{Rovio Entertainment}, the video game company. We build the benchmarking system on top of \textit{Rovio} use-case. One use-case is related with analytics on windowed aggregation, calculating the average session time of users within specified geo locations. Another use-case is associated with windowed join, calculating the time lag between clients and company services. 

In this work we propose the benchmarking system to assess the performance of three major, open source and community driven streaming engines, being Apache Storm, Apache Spark and Apache Flink.  Throughout  the tests, we use latency and throughput as the major performance indicators. Latency is the time required to calculate the final result, throughput on the other hand determines the number of successful calculations per unit time. In stream data processing, it is number of tuples per unit time the engine can ingest for further process. We use two \textit{Rovio} use-cases for experiments. 


There are numerous open challenges related with measuring main Key Performance Indicators (KPIs) in stream data processing engines. For example, there is not clear definition for the latency of stateful operator within SDPS.   That is,  the stream is theoretically infinite and there is no final output. While the previous works use checkpointing each event's ingestion and output time, this can lead to misleading results and escalade the driver's complexity. Moreover, while benchmarking SDPSs, it is crucial to separate completely the driver and units under test as otherwise, the results can be biased. Furthermore, the driver should be flexible enough to support different policies to handle system specific features. For example, back-pressure is characteristic feature of SDPSs and while calculating system's sustainability it must be taken into consideration.
Most importantly, keeping the benchmarking system simple while solving the challenges listed above is imperative. 

%\para{Matrix Blocking Through Joins.}
In this paper, we overcome the challenges listed above. The proposed solution is generic, has simple design with clear semantics and can be applied to any SDPS. We keep the design of a solution as simple as possible. The main intuition is to simulate an environment in which we can calculate the KIPs more precisely and with minimum influence of side factors. We design experiments on top of $Rovio$ use-cases.

The main contributions of this paper are listed above:
\begin{itemize}  
\item We first introduce the term sustainability of SDPS and give its definition. The stream data processing engine is sustainable with a given input if it can process it within user-defined quality boundaries.  For example, user can define custom logic which can handle back-pressure while measuring system's sustainability. 
\item  We first  introduce  the mechanism to measure the latency in stateful operators is SDPS. We applied the proposed method with partitioned windowed aggregation and partitioned windowed join use cases. 
\item We accomplish the above contributions with a simple system design in which the KPI measurements are clearly separated from units under test. 
\item We test the proposed benchmark on Storm, Spark and Flink with industry use-cases of $Rovio$.
\end{itemize}

The remainder of paper is organized as follows. In Section \ref{rel}, related benchmarks in stream data processing and in general big data analytics are studied. The preliminary and background information about the engines being tested are analyzed in Section \ref{pre}. The detailed interpretation of open challenges  and their importance are discussed in Section \ref{chal}. We provide the design of benchmark system, the use-cases and KPIs in Section \ref{des}. After evaluations in Section \ref{eval} we conclude in Section \ref{conc}.
